@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@misc{roberts2022autowsbench101,
      title={AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels}, 
      author={Nicholas Roberts and Xintong Li and Tzu-Heng Huang and Dyah Adila and Spencer Schoenberg and Cheng-Yu Liu and Lauren Pick and Haotian Ma and Aws Albarghouthi and Frederic Sala},
      year={2022},
      eprint={2208.14362},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2022_b1efde53,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{nushi2018accountable,
      title={Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure}, 
      author={Besmira Nushi and Ece Kamar and Eric Horvitz},
      year={2018},
      eprint={1809.07424},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{singla2021understanding,
      title={Understanding Failures of Deep Networks via Robust Feature Extraction}, 
      author={Sahil Singla and Besmira Nushi and Shital Shah and Ece Kamar and Eric Horvitz},
      year={2021},
      eprint={2012.01750},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{geng2023flexible,
      title={Flexible Grammar-Based Constrained Decoding for Language Models}, 
      author={Saibo Geng and Martin Josifosky and Maxime Peyrard and Robert West},
      year={2023},
      eprint={2305.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{honnibal2020spacy,
  added-at = {2022-05-10T23:39:24.000+0200},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  biburl = {https://www.bibsonomy.org/bibtex/2616669ca18ac051794c0459373696942/albinzehe},
  doi = {10.5281/zenodo.1212303},
  interhash = {2d1b3a0bb97e51df1b88d8852cd5ac01},
  intrahash = {616669ca18ac051794c0459373696942},
  keywords = {kallimachos library nlp paper-2022-lotr spacy},
  timestamp = {2022-07-05T09:43:17.000+0200},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020
}

@misc{wang2023selfinstruct,
      title={Self-Instruct: Aligning Language Models with Self-Generated Instructions}, 
      author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
      year={2023},
      eprint={2212.10560},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{jones_reprap_2011, 
    title={RepRap – the replicating rapid prototyper}, 
    volume={29}, 
    DOI={10.1017/S026357471000069X}, 
    number={1}, 
    journal={Robotica}, 
    publisher={Cambridge University Press}, 
    author={Jones, Rhys and Haufe, Patrick and Sells, Edward and Iravani, Pejman and Olliver, Vik and Palmer, Chris and Bowyer, Adrian}, 
    year={2011}, 
    pages={177–191}
}


@misc{lai2017race,
    title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
    author={Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard Hovy},  
    year={2017},
    eprint={1704.04683},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/1704.04683}
}


@InProceedings{pmlr-v101-liang19a,
  title = 	 {A New Multi-choice Reading Comprehension Dataset for Curriculum Learning},
  author =       {Liang, Yichan and Li, Jianheng and Yin, Jian},
  booktitle = 	 {Proceedings of The Eleventh Asian Conference on Machine Learning},
  pages = 	 {742--757},
  year = 	 {2019},
  editor = 	 {Lee, Wee Sun and Suzuki, Taiji},
  volume = 	 {101},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--19 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v101/liang19a/liang19a.pdf},
  url = 	 {https://proceedings.mlr.press/v101/liang19a.html},
  abstract = 	 {The past few years have witnessed the rapid development of machine reading comprehension (MRC), especially the challenging sub-task, multiple-choice reading comprehension (MCRC). And the release of large scale datasets promotes the research in this field. Yet previous methods have already achieved high accuracy of the MCRC datasets, \textit{e.g.} RACE. It’s necessary to propose a more difficult dataset which needs more reasoning and inference for evaluating the understanding capability of new methods. To respond to such demand, we present RACE-C, a new multi-choice reading comprehension dataset collected from college English examinations in China. And further we integrate it with RACE-M and RACE-H, collected by {{Lai et&nbsp;al.}} ({2017}) from middle and high school exams respectively, to extend RACE to be RACE++. Based on RACE++, we propose a three-stage curriculum learning framework, which is able to use the best of the characteristic that the difficulty level within these three sub-datasets is in ascending order. Statistics show the higher difficulty level of our collected dataset, RACE-C, compared to RACE’s two sub-datasets, \textit{i.e.}, RACE-M and RACE-H. And experimental results demonstrate that our proposed three-stage curriculum learning approach improves the performance of the machine reading comprehension model to an extent.}
}

@online{visjs,
    author    = {vis.js developers},
    title     = {vis.js community edition},
    year      = {2023},
    url       = {https://visjs.org/},
    urldate   = {2023-10-04}
}

@online{ThoughtGraph,
   author={ThoughtGraph~developers},
   title={ThoughtGraph GitHub repository},
   year={2023},
   url={https://github.com/rmhorton/ThoughtGraph},
}


@misc{ wiki:gunning_fog,
  author = {{Wikipedia authors}},
  title = "Gunning fog index --- {W}ikipedia{,} The Free Encyclopedia",
  year = "2023",
  url = "https://en.wikipedia.org/wiki/Gunning_fog_index",
  note = "[Online; accessed 4-October-2023]"
}