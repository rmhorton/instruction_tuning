{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.linalg as la\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df = pd.read_parquet('./data/race_train_prepared.parquet')\n",
    "race_test_df = pd.read_parquet('./data/race_test_prepared.parquet')\n",
    "dolly_cqa_df = pd.read_parquet('./data/dolly_cqa_prepared.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability\n",
    "\n",
    "def compute_readability(text):\n",
    "    r = Readability(text)\n",
    "    gf = r.gunning_fog()\n",
    "    gf_score = gf.score\n",
    "    return gf_score\n",
    "\n",
    "race_train_df['gf_score'] = race_train_df['text'].apply(lambda x: compute_readability(x))\n",
    "race_test_df['gf_score'] = race_test_df['text'].apply(lambda x: compute_readability(x))\n",
    "dolly_cqa_df['gf_score'] = dolly_cqa_df['text'].apply(lambda x: compute_readability(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df.to_parquet('./data/race_train_df_readability.parquet', index=False)\n",
    "race_test_df.to_parquet('./data/race_test_df_readability.parquet', index=False)\n",
    "dolly_cqa_df.to_parquet('./data/dolly_cqa_df_readability.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df = pd.read_parquet('./data/race_train_df_readability.parquet')\n",
    "race_test_df = pd.read_parquet('./data/race_test_df_readability.parquet')\n",
    "dolly_cqa_df = pd.read_parquet('./data/dolly_cqa_df_readability.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df['gf_score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_cqa_df['gf_score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_train_df = race_train_df[race_train_df['gf_score'] >= 5]\n",
    "race_train_df = race_train_df[race_train_df['gf_score'] <= 35]\n",
    "\n",
    "# race_test_df = race_test_df[race_test_df['gf_score'] >= 5]\n",
    "race_test_df = race_test_df[race_test_df['gf_score'] <= 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df['gf_score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def equal_frequency_binning(data, num_bins):\n",
    "#     \"\"\"\n",
    "#     Bucketize the data into bins with approximately equal number of data points.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - data (list or np.array): The data to be bucketized.\n",
    "#     - num_bins (int): Number of bins desired.\n",
    "    \n",
    "#     Returns:\n",
    "#     - bins (list of tuples): List of intervals representing the bins.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if not isinstance(data, np.ndarray):\n",
    "#         data = np.array(data)\n",
    "    \n",
    "#     # Sort the data\n",
    "#     sorted_data = np.sort(data)\n",
    "    \n",
    "#     # Calculate bin edges using quantiles\n",
    "#     bin_edges = [np.percentile(sorted_data, i) for i in np.linspace(0, 100, num_bins+1)]\n",
    "    \n",
    "#     # Create bins as tuples of (start, end)\n",
    "#     bins = [(bin_edges[i], bin_edges[i+1]) for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "#     return bins\n",
    "\n",
    "# def bin_data(data, bins, bin_names):\n",
    "#     \"\"\"\n",
    "#     Convert continuous data into categorical data using specified bins and bin names.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - data (list or np.array): The data to be bucketized.\n",
    "#     - bins (list of tuples): List of intervals representing the bins.\n",
    "#     - bin_names (list of str): Names for each bin.\n",
    "    \n",
    "#     Returns:\n",
    "#     - categorical_data (list of str): Categorical representation of the data.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if not isinstance(data, np.ndarray):\n",
    "#         data = np.array(data)\n",
    "    \n",
    "#     if len(bins) != len(bin_names):\n",
    "#         raise ValueError(\"Number of bins and bin names should be the same.\")\n",
    "    \n",
    "#     # Initialize an empty list to store the categorical data\n",
    "#     categorical_data = []\n",
    "    \n",
    "#     # Loop over each data point to assign it to a bin\n",
    "#     for value in data:\n",
    "#         assigned = False\n",
    "#         for i, (start, end) in enumerate(bins):\n",
    "#             if start <= value < end or (i == len(bins) - 1 and value == end):\n",
    "#                 categorical_data.append(bin_names[i])\n",
    "#                 assigned = True\n",
    "#                 break\n",
    "#         if not assigned:\n",
    "#             categorical_data.append('Unknown')\n",
    "    \n",
    "#     return categorical_data\n",
    "\n",
    "def bucketize_by_quantiles(df, column_name, n_buckets):\n",
    "    \"\"\"\n",
    "    Bucketizes the continuous values in the specified column of the DataFrame into\n",
    "    n_buckets using quantiles, and returns a new categorical column with the bucketized values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column_name: Name of the column with continuous values\n",
    "    - n_buckets: Number of buckets to create\n",
    "    \n",
    "    Returns:\n",
    "    - A new column with bucketized values\n",
    "    \"\"\"\n",
    "    bucket_labels = [f\"L{i+1}\" for i in range(n_buckets)]\n",
    "    return pd.qcut(df[column_name], q=n_buckets, labels=bucket_labels)\n",
    "\n",
    "# def bucketize(df, column_name):\n",
    "#     # Create an ordered index based on the values in the column\n",
    "#     ordered_index = df[column_name].sort_values().index\n",
    "    \n",
    "#     # Round the continuous values to nearest integer\n",
    "#     df['rounded'] = df[column_name].round().astype(int)\n",
    "    \n",
    "#     # Apply rolling window on ordered values to compute the middle value for each window\n",
    "#     bucketized = df.loc[ordered_index, 'rounded'].rolling(window=3, center=True).apply(lambda x: x[1], raw=True)\n",
    "    \n",
    "#     # Fill NaN values for beginning edge with the first valid bucket value\n",
    "#     first_valid_bucket = bucketized.dropna().iloc[0]\n",
    "#     bucketized.iloc[:bucketized.first_valid_index()] = first_valid_bucket\n",
    "    \n",
    "#     # Fill NaN values for ending edge with the last valid bucket value\n",
    "#     last_valid_bucket = bucketized.dropna().iloc[-1]\n",
    "#     bucketized.iloc[bucketized.last_valid_index()+1:] = last_valid_bucket\n",
    "    \n",
    "#     # Convert the bucketized values to integers\n",
    "#     bucketized = bucketized.astype(int)\n",
    "    \n",
    "#     # Convert the integer values to strings by prepending \"L\" to them\n",
    "#     bucketized = 'L' + bucketized.astype(str)\n",
    "    \n",
    "#     # Restore the original order\n",
    "#     bucketized = bucketized.reindex(df.index)\n",
    "    \n",
    "#     # Drop the temporary 'rounded' column\n",
    "#     df.drop('rounded', axis=1, inplace=True)\n",
    "    \n",
    "#     return bucketized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins_race_train = equal_frequency_binning(race_train_df['gf_score'], num_bins=15)\n",
    "# race_train_df['gf_score_categ'] = bin_data(race_train_df['gf_score'], bins_race_train, ['L' + str(i) for i in range(1,16)])\n",
    "\n",
    "# bins_race_test = equal_frequency_binning(race_test_df['gf_score'], num_bins=15)\n",
    "# race_test_df['gf_score_categ'] = bin_data(race_test_df['gf_score'], bins_race_test, ['L' + str(i) for i in range(1,16)])\n",
    "\n",
    "# bins_dolly = equal_frequency_binning(dolly_cqa_df['gf_score'], num_bins=15)\n",
    "# dolly_cqa_df['gf_score_categ'] = bin_data(dolly_cqa_df['gf_score'], bins_dolly, ['L' + str(i) for i in range(1,16)])\n",
    "\n",
    "# race_train_df['gf_score_categ'] = race_train_df['gf_score'].apply(lambda x: str(round(x)))\n",
    "# race_test_df['gf_score_categ'] = race_test_df['gf_score'].apply(lambda x: str(round(x)))\n",
    "# dolly_cqa_df['gf_score_categ'] = dolly_cqa_df['gf_score'].apply(lambda x: str(round(x)))\n",
    "\n",
    "race_train_df['gf_score_categ'] = bucketize_by_quantiles(df=race_train_df, column_name='gf_score', n_buckets=10)\n",
    "race_test_df['gf_score_categ'] = bucketize_by_quantiles(df=race_test_df, column_name='gf_score', n_buckets=10)\n",
    "dolly_cqa_df['gf_score_categ'] = bucketize_by_quantiles(df=dolly_cqa_df, column_name='gf_score', n_buckets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = race_train_df['gf_score_categ'].unique()\n",
    "labels = sorted(labels, key=lambda x: int(x[1:]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = race_test_df['gf_score_categ'].unique()\n",
    "labels = sorted(labels, key=lambda x: int(x[1:]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dolly_cqa_df['gf_score_categ'].unique()\n",
    "labels = sorted(labels, key=lambda x: int(x[1:]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_classifier(df, embeddings_col, y_col):\n",
    "    # Convert the list of lists in X_colname to a numpy array\n",
    "    X = np.array(df[embeddings_col].tolist())\n",
    "    \n",
    "    y = df[y_col]\n",
    "    \n",
    "    # Label encoding the target variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    # Initialize LR classifier\n",
    "    # clf = LogisticRegressionCV(cv=5, scoring='f1_macro', max_iter=1000, n_jobs=-1)\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=20000)\n",
    "\n",
    "    # Fit the classifier\n",
    "    clf.fit(X, y_encoded)\n",
    "\n",
    "    return le, clf\n",
    "\n",
    "def get_predictions(df, le, clf, embeddings_col, y_col):\n",
    "    # Convert the list of lists in X_colname to a numpy array\n",
    "    X = np.array(df[embeddings_col].tolist())\n",
    "    \n",
    "    y = df[y_col]\n",
    "\n",
    "    # Label encoding the target variable\n",
    "    y_encoded = le.transform(y)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_encoded = clf.predict(X)\n",
    "    y_pred_proba = clf.predict_proba(X)\n",
    "\n",
    "    # Reverse-transform the predicted and actual labels to their original values\n",
    "    y_pred = le.inverse_transform(y_pred_encoded)\n",
    "    y_test = le.inverse_transform(y_encoded)\n",
    "\n",
    "    # Printing the classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    labels = df[y_col].unique().tolist()\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "def create_clusters(df, n_clusters):\n",
    "    clusters = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto').fit( [v for v in df['embeddings_bge_large']])\n",
    "    return clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le, clf = train_lr_classifier(df=race_train_df, embeddings_col='embeddings_bge_large', y_col='difficulty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_proba = get_predictions(df=race_test_df, le=le, clf=clf, embeddings_col='embeddings_bge_large', y_col='difficulty')\n",
    "\n",
    "race_test_df['predicted_difficulty'] = y_pred\n",
    "race_test_df['difficulty_proba_C'] = y_pred_proba[:,0]\n",
    "race_test_df['difficulty_proba_H'] = y_pred_proba[:,1]\n",
    "race_test_df['difficulty_proba_M'] = y_pred_proba[:,2]\n",
    "\n",
    "race_test_df['cluster'] = create_clusters(df=race_test_df, n_clusters=10)\n",
    "\n",
    "race_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dolly_cqa_df['embeddings_bge_large'].tolist())\n",
    "y_pred_encoded = clf.predict(X)\n",
    "y_pred_proba = clf.predict_proba(X)\n",
    "y_pred = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "dolly_cqa_df['predicted_difficulty'] = y_pred\n",
    "dolly_cqa_df['difficulty_proba_C'] = y_pred_proba[:,0]\n",
    "dolly_cqa_df['difficulty_proba_H'] = y_pred_proba[:,1]\n",
    "dolly_cqa_df['difficulty_proba_M'] = y_pred_proba[:,2]\n",
    "\n",
    "dolly_cqa_df['cluster'] = create_clusters(df=dolly_cqa_df, n_clusters=10)\n",
    "\n",
    "dolly_cqa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # sns.boxplot(x='gf_score_categ', y='gf_score', data=df, order=['L' + str(i) for i in range(1,16)])\n",
    "    \n",
    "    # labels = df['gf_score_categ'].unique().tolist()\n",
    "    # labels.sort(key=int)\n",
    "    # sns.boxplot(x='gf_score_categ', y='gf_score', data=df, order=labels)\n",
    "\n",
    "    labels = df['gf_score_categ'].unique()\n",
    "    labels = sorted(labels, key=lambda x: int(x[1:]))\n",
    "    sns.boxplot(x='gf_score_categ', y='gf_score', data=df, order=labels)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df=race_test_df, title='Boxplots of gf_score by gf_score_categ - RACE Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df=dolly_cqa_df, title='Boxplots of gf_score by gf_score_categ - Dolly Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mosaic(df, title):\n",
    "#     # Set the aesthetic style of the plots\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "\n",
    "#     # Plotting Mosaic Plot\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     mosaic(df.sort_values('predicted_difficulty'), ['predicted_difficulty', 'gf_score_categ'], title=title)\n",
    "#     plt.show()\n",
    "\n",
    "def plot_stacked(df, title):\n",
    "    # Plotting Stacked Bar Plot\n",
    "    contingency_table = pd.crosstab(df['predicted_difficulty'], df['gf_score_categ'])\n",
    "    contingency_table = contingency_table.reindex(['M', 'H', 'C'])\n",
    "    \n",
    "    # contingency_table = contingency_table.reindex(columns=['L' + str(i) for i in range(1,16)])\n",
    "    \n",
    "    # labels = df['gf_score_categ'].unique().tolist()\n",
    "    # labels.sort(key=int)\n",
    "    # contingency_table = contingency_table.reindex(columns=labels)\n",
    "\n",
    "    labels = df['gf_score_categ'].unique()\n",
    "    labels = sorted(labels, key=lambda x: int(x[1:]))\n",
    "    contingency_table = contingency_table.reindex(columns=labels)\n",
    "\n",
    "    # Normalizing the contingency table to show percentages\n",
    "    contingency_table_percentage = contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    contingency_table_percentage.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.show()\n",
    "    return contingency_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mosaic(df=race_test_df, title='Mosaic Plot of difficulty vs gf_score - RACE Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mosaic(df=dolly_cqa_df, title='Mosaic Plot of difficulty vs gf_score - Dolly Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table_race = plot_stacked(df=race_test_df, title='Stacked Bar Plot of difficulty vs gf_score - RACE Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table_dolly = plot_stacked(df=dolly_cqa_df, title='Stacked Bar Plot of difficulty vs gf_score - Dolly Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(contingency_table):\n",
    "    # Chi-Squared Test\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Cramér's V calculation\n",
    "    chi2_val = chi2_contingency(contingency_table)[0]\n",
    "    n = contingency_table.sum().sum()\n",
    "    cramers_v = (chi2_val / (n * min(contingency_table.shape[0]-1, contingency_table.shape[1]-1)))**0.5\n",
    "\n",
    "    return chi2, p, cramers_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, cramers_v = compute_correlation(contingency_table_race)\n",
    "print('Chi-squared: ', chi2, ' | p-value: ', p)\n",
    "print(\"Cramer's V: \", cramers_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, cramers_v = compute_correlation(contingency_table_dolly)\n",
    "print('Chi-squared: ', chi2, ' | p-value: ', p)\n",
    "print(\"Cramer's V: \", cramers_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correspondence_analysis(contingency_table):\n",
    "    # Calculate row and column marginal totals\n",
    "    row_totals = contingency_table.sum(axis=1)\n",
    "    col_totals = contingency_table.sum(axis=0)\n",
    "    \n",
    "    # Grand total\n",
    "    grand_total = contingency_table.sum().sum()\n",
    "    \n",
    "    # Expected frequencies under independence\n",
    "    expected = np.outer(row_totals, col_totals) / grand_total\n",
    "    \n",
    "    # Standardized residuals\n",
    "    S = (contingency_table - expected) / np.sqrt(expected)\n",
    "    \n",
    "    # Singular value decomposition\n",
    "    U, D, Vt = la.svd(S, full_matrices=False)\n",
    "    \n",
    "    # Row and column coordinates\n",
    "    row_coords = U * D\n",
    "    col_coords = Vt.T * D\n",
    "\n",
    "    # Percentage of variance explained by each dimension\n",
    "    explained_variance = (D**2) / sum(D**2)\n",
    "    \n",
    "    return row_coords, col_coords, explained_variance\n",
    "\n",
    "\n",
    "def plot_correspondance_analysis(contingency_table, title, row_label, column_label):\n",
    "    # Perform correspondence analysis\n",
    "    row_coords, col_coords, explained_variance = correspondence_analysis(contingency_table)\n",
    "\n",
    "    # Define row colors\n",
    "    row_colors = ['red', 'green', 'blue']\n",
    "\n",
    "    # Plotting the bi-plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(row_coords[:, 0], row_coords[:, 1], color='red', marker='o', label=row_label, alpha=0.6)\n",
    "    plt.scatter(col_coords[:, 0], col_coords[:, 1], color='blue', marker='s', label=column_label, alpha=0.6)\n",
    "\n",
    "    # Annotate row and column labels\n",
    "    for idx, label in enumerate(contingency_table.index):\n",
    "        plt.annotate(label, (row_coords[idx, 0] + 0.05, row_coords[idx, 1] + 0.05), color='red')\n",
    "    for idx, label in enumerate(contingency_table.columns):\n",
    "        plt.annotate(label, (col_coords[idx, 0] + 0.05, col_coords[idx, 1] + 0.05), color='blue')\n",
    "\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.axvline(0, color='gray', linestyle='--')\n",
    "    plt.title(title)\n",
    "   \n",
    "    plt.xlabel('Dimension 1: {:.2f}% Explained Variance'.format(explained_variance[0]*100))\n",
    "    plt.ylabel('Dimension 2: {:.2f}% Explained VAriance'.format(explained_variance[1]*100))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# def plot_correspondance_analysis(contingency_table, title):\n",
    "#     # Perform correspondence analysis\n",
    "#     row_coords, col_coords, explained_variance = correspondence_analysis(contingency_table)\n",
    "\n",
    "#     # Define row colors\n",
    "#     row_colors = ['red', 'green', 'blue']\n",
    "\n",
    "#     # Plotting the bi-plot\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "\n",
    "#     # Plot and annotate rows\n",
    "#     for idx, label in enumerate(contingency_table.index):\n",
    "#         plt.scatter(row_coords[idx, 0], row_coords[idx, 1], color=row_colors[idx], marker='o', alpha=0.6)\n",
    "#         plt.annotate(label, (row_coords[idx, 0] + 0.05, row_coords[idx, 1] + 0.05), color=row_colors[idx])\n",
    "    \n",
    "#     # Plot and annotate columns based on the closest row point\n",
    "#     for idx_col, label_col in enumerate(contingency_table.columns):\n",
    "#         distances = np.linalg.norm(row_coords - col_coords[idx_col], axis=1)\n",
    "#         closest_row_idx = np.argmin(distances)\n",
    "#         plt.scatter(col_coords[idx_col, 0], col_coords[idx_col, 1], color=row_colors[closest_row_idx], marker='s', alpha=0.6)\n",
    "#         plt.annotate(label_col, (col_coords[idx_col, 0] + 0.05, col_coords[idx_col, 1] + 0.05), color=row_colors[closest_row_idx])\n",
    "\n",
    "#     plt.axhline(0, color='gray', linestyle='--')\n",
    "#     plt.axvline(0, color='gray', linestyle='--')\n",
    "#     plt.title(title)\n",
    "   \n",
    "#     plt.xlabel('Dimension 1: {:.2f}%'.format(explained_variance[0]*100))\n",
    "#     plt.ylabel('Dimension 2: {:.2f}%'.format(explained_variance[1]*100))\n",
    "    \n",
    "#     # Create legend for row colors\n",
    "#     legend_elements1 = [plt.Line2D([0], [0], color=row_colors[idx], marker='o', linestyle='', markersize=10, label=label) \n",
    "#                         for idx, label in enumerate(contingency_table.index)]\n",
    "    \n",
    "#     # Create legend for row and column labels\n",
    "#     legend_elements2 = [plt.Line2D([0], [0], color='black', marker='o', linestyle='', markersize=10, label='Rows (difficulty)'),\n",
    "#                         plt.Line2D([0], [0], color='black', marker='s', linestyle='', markersize=10, label='Columns (gf_score_categ)')]\n",
    "    \n",
    "#     # Place both legends outside the plot, top-right\n",
    "#     legend1 = plt.legend(handles=legend_elements1, loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"Rows (difficulty)\")\n",
    "#     legend2 = plt.legend(handles=legend_elements2, loc=\"upper left\", bbox_to_anchor=(1, 0.75))\n",
    "    \n",
    "#     # Add the first legend manually to the current Axes\n",
    "#     plt.gca().add_artist(legend1)\n",
    "    \n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correspondance_analysis(contingency_table=contingency_table_race, title='Correspondence Analysis Bi-plot - RACE Dataset',\n",
    "                             row_label='Rows (predicted reading comprehension)', column_label='Columns (computed readability score)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correspondance_analysis(contingency_table=contingency_table_dolly, title='Correspondence Analysis Bi-plot - Dolly Dataset',\n",
    "                             row_label='Rows (predicted reading comprehension)', column_label='Columns (computed readability score)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['predicted_difficulty'].isin(['C'])]\n",
    "df = df[df['gf_score_categ']=='L8']\n",
    "\n",
    "for text, categ in zip(df['text'], df['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_percentage_table(*series_list, series_names=None, order=None):\n",
    "    \"\"\"\n",
    "    Generates a table with the percentage of instances for each category level, for each series.\n",
    "    \n",
    "    Parameters:\n",
    "    *series_list: One or more pandas Series.\n",
    "    series_names: List of names for the series. Defaults to ['Series 1', 'Series 2', ...]\n",
    "    order: List specifying the desired order of category levels in the table.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A table with the percentage of instances for each category level, for each series.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not series_names:\n",
    "        series_names = [f\"Series {i+1}\" for i in range(len(series_list))]\n",
    "    \n",
    "    if len(series_list) != len(series_names):\n",
    "        raise ValueError(\"The number of series and series names must match.\")\n",
    "    \n",
    "    # Initialize a DataFrame to store the results\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # For each series, calculate the percentage distribution of the categories\n",
    "    for s, name in zip(series_list, series_names):\n",
    "        df[name] = s.value_counts(normalize=True).mul(100).round().astype(int).astype(str) + '%'\n",
    "    \n",
    "    # Fill NaN with '0%' (this means that a particular category was not present in a series)\n",
    "    df.fillna('0%', inplace=True)\n",
    "    \n",
    "    # If an order is provided, reorder the rows accordingly\n",
    "    if order:\n",
    "        df = df.reindex(order).fillna('0%')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_percentage_table(race_train_df['difficulty'], race_test_df['difficulty'], dolly_cqa_df['predicted_difficulty'],\n",
    "                          series_names=['RACE train', 'RACE test', 'Dolly'], order=['M', 'H', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_scores(df, categ, title, legend_title):\n",
    "  \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    labels = df[categ].unique()\n",
    "    labels = sorted(labels, key=lambda x: int(x[1:]))\n",
    "    # labels = ['L' + str(i) for i in range(1,16)]\n",
    "\n",
    "    sns.countplot(data=df, x='cluster', hue=categ, hue_order=labels, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Cluster')\n",
    "    ax.set_ylabel('Count')\n",
    "    \n",
    "    # Place the legend outside the plot, to the right\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    legend.set_title(legend_title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_scores(df=dolly_cqa_df, categ='gf_score_categ', title='Readability Level Counts by Cluster',\n",
    "                    legend_title='Gunning Fog Score Levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['cluster']==2]\n",
    "df10 = df[df['gf_score_categ']=='L1']\n",
    "\n",
    "for text, categ in zip(df10['text'], df10['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['cluster']==8]\n",
    "df1 = df[df['gf_score_categ']=='L10']\n",
    "\n",
    "for text, categ in zip(df1['text'], df1['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['predicted_difficulty'].isin(['M'])]\n",
    "plot_cluster_scores(df=df, categ='gf_score_categ', title='Readability Level Counts by CLuster for Reading Comprehension Level \"M\"',\n",
    "                    legend_title='Gunning Fog Score Levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df[df['cluster']==0]\n",
    "df0 = df0[df0['gf_score_categ']=='L8']\n",
    "\n",
    "for text, categ in zip(df0['text'], df0['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df[df['cluster']==8]\n",
    "df8 = df8[df8['gf_score_categ'].isin(['L6', 'L7'])]\n",
    "\n",
    "for text, categ in zip(df8['text'], df8['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['predicted_difficulty'].isin(['H'])]\n",
    "plot_cluster_scores(df=df, categ='gf_score_categ', title='Readability Level Counts by Cluster for Reading Comprehension Level \"H\"',\n",
    "                    legend_title='Gunning Fog Score Levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df[df['cluster']==8]\n",
    "df8 = df8[df8['gf_score_categ']=='L1']\n",
    "\n",
    "for text, categ in zip(df8['text'], df8['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['cluster']==1]\n",
    "df1 = df1[df1['gf_score_categ']=='L10']\n",
    "\n",
    "for text, categ in zip(df1['text'], df1['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['predicted_difficulty'].isin(['C'])]\n",
    "plot_cluster_scores(df=df, categ='gf_score_categ', title='Readability Level Counts by CLuster for Reading Comprehension Level \"C\"',\n",
    "                    legend_title='Gunning Fog Score Levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['cluster']==1]\n",
    "df1 = df1[df1['gf_score_categ']=='L10']\n",
    "\n",
    "for text, categ in zip(df1['text'], df1['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['cluster']==2]\n",
    "df2 = df2[df2['gf_score_categ']=='L1']\n",
    "\n",
    "for text, categ in zip(df2['text'], df2['gf_score_categ']):\n",
    "    print('gf_score_categ:', categ, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_scores(df, categ, title, legend_title):\n",
    "  \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    labels = ['M', 'H', 'C']\n",
    "\n",
    "    sns.countplot(data=df, x='cluster', hue=categ, hue_order=labels, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Cluster')\n",
    "    ax.set_ylabel('Count')\n",
    "    \n",
    "    # Place the legend outside the plot, to the right\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    legend.set_title(legend_title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['gf_score_categ'].isin(['L10'])]\n",
    "plot_cluster_scores(df=df, categ='predicted_difficulty', title='Counts by CLuster for Readability Level \"L10\"',\n",
    "                    legend_title='Predicted Reading Comprehension Levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dolly_cqa_df[dolly_cqa_df['gf_score_categ'].isin(['L1'])]\n",
    "plot_cluster_scores(df=df, categ='predicted_difficulty', title='Counts by CLuster for Readability Level \"L1\"',\n",
    "                    legend_title='Predicted Reading Comprehension Levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['cluster']==2]\n",
    "df2 = df2[df2['predicted_difficulty']=='C']\n",
    "\n",
    "for text, level in zip(df2['text'], df2['predicted_difficulty']):\n",
    "    print('predicted difficulty:', level, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df[df['cluster']==8]\n",
    "df8 = df8[df8['predicted_difficulty']=='H']\n",
    "\n",
    "for text, level in zip(df8['text'], df8['predicted_difficulty']):\n",
    "    print('predicted difficulty:', level, '\\n')\n",
    "    print(text, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, categ):\n",
    "    # List of continuous columns\n",
    "    columns = ['difficulty_proba_M', 'difficulty_proba_C', 'difficulty_proba_H']\n",
    "    \n",
    "    # Setting up the figure and axes\n",
    "    fig, axs = plt.subplots(nrows=3, figsize=(10, 15))\n",
    "    \n",
    "    # Looping through each column and plotting\n",
    "    for i, column in enumerate(columns):\n",
    "        sns.boxplot(x=categ, y=column, data=df, ax=axs[i], order=['L' + str(i) for i in range(1,10)])\n",
    "        axs[i].set_title(f\"Boxplot of {column} by {categ}\")\n",
    "    \n",
    "    # Displaying the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(race_test_df, 'gf_score_categ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(dolly_cqa_df, 'gf_score_categ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
