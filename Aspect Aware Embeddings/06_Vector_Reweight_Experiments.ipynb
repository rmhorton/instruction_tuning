{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_cqa_df = pd.read_parquet('./data/dolly_cqa_prepared.parquet')\n",
    "dolly_cqa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_pattern = r'\\baseball(?!: bat)|basketball|badminton|tennis(?!: shoe)|soccer|futbol|football (?!: stadium)'\n",
    "domain_flags = [bool(regex.search(domain_pattern, sent, regex.IGNORECASE)) for sent in dolly_cqa_df['text']]\n",
    "\n",
    "task_pattern = r'what (is|are)'\n",
    "task_flags = [bool(regex.search(task_pattern, sent, regex.IGNORECASE)) for sent in dolly_cqa_df['text']]\n",
    "\n",
    "structure_pattern = r'\\b(given a|given the|the given|based on|reference text|as a reference|the following text)'\n",
    "structure_flags = [bool(regex.search(structure_pattern, sent, regex.IGNORECASE)) for sent in dolly_cqa_df['text']]\n",
    "\n",
    "dolly_cqa_df['domain_flag'] = domain_flags\n",
    "dolly_cqa_df['task_flag'] = task_flags\n",
    "dolly_cqa_df['structure_flag'] = structure_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_classifier(df, embeddings_col, y_col):\n",
    "    # Convert the list of lists in X_colname to a numpy array\n",
    "    X = np.array(df[embeddings_col].tolist())\n",
    "    \n",
    "    y = df[y_col]\n",
    "    \n",
    "    # Initialize LR classifier\n",
    "    clf = LogisticRegressionCV(cv=5, scoring='f1_macro', max_iter=10000, n_jobs=-1)\n",
    "\n",
    "    # Fit the classifier\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_domain = train_lr_classifier(df=dolly_cqa_df, embeddings_col='embeddings_mini_lm', y_col='domain_flag')\n",
    "clf_task = train_lr_classifier(df=dolly_cqa_df, embeddings_col='embeddings_mini_lm', y_col='task_flag')\n",
    "clf_structure = train_lr_classifier(df=dolly_cqa_df, embeddings_col='embeddings_mini_lm', y_col='structure_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df, clf, embeddings_col, y_col):\n",
    "    # Convert the list of lists in X_colname to a numpy array\n",
    "    X = np.array(df[embeddings_col].tolist())\n",
    "\n",
    "    # Normalize X\n",
    "    X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    \n",
    "    y = df[y_col]\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # Printing the classification report\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(df=dolly_cqa_df, clf=clf_domain, embeddings_col='embeddings_mini_lm', y_col='domain_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(df=dolly_cqa_df, clf=clf_task, embeddings_col='embeddings_mini_lm', y_col='task_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(df=dolly_cqa_df, clf=clf_structure, embeddings_col='embeddings_mini_lm', y_col='structure_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf_task\n",
    "\n",
    "dolly_cqa_df['score'] = model.predict_proba(X=np.array(dolly_cqa_df['embeddings_mini_lm'].tolist()))[:,1]\n",
    "dolly_cqa_df['w_embeddings'] = dolly_cqa_df['embeddings_mini_lm'].apply(lambda x: x * model.coef_[0])\n",
    "dolly_cqa_df['iw_embeddings'] = dolly_cqa_df['embeddings_mini_lm'].apply(lambda x: x / model.coef_[0])\n",
    "\n",
    "seed=0\n",
    "uw_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\").fit( [v for v in dolly_cqa_df['embeddings_mini_lm']])\n",
    "w_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\").fit( [v for v in dolly_cqa_df['w_embeddings']])\n",
    "iw_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\").fit( [v for v in dolly_cqa_df['iw_embeddings']])\n",
    "\n",
    "dolly_cqa_df['uw_kmeans'] = uw_kmeans.labels_\n",
    "dolly_cqa_df['w_kmeans'] = w_kmeans.labels_\n",
    "dolly_cqa_df['iw_kmeans'] = iw_kmeans.labels_\n",
    "\n",
    "cluster_df = dolly_cqa_df[['uw_kmeans', 'w_kmeans', 'iw_kmeans', 'score']]\n",
    "cluster_df = pd.melt(cluster_df, id_vars=['score'], var_name='weighting', value_name='cluster_id')\n",
    "cluster_df = cluster_df.groupby([\"weighting\", \"cluster_id\"]).agg({'score': ['mean', 'var']}).reset_index()\n",
    "cluster_df.columns = ['weighting', 'cluster_id', 'mean_score', 'var_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(5, 3)})\n",
    "fig, axes = plt.subplots(2, 1, sharey=True)\n",
    "\n",
    "sns.stripplot(x='mean_score', y='weighting', data=cluster_df, jitter=True, hue='weighting', dodge=True, ax=axes[0])\n",
    "axes[0].legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "axes[0].set(ylabel=None)\n",
    "axes[0].set(xticklabels=[])\n",
    "axes[0].set(xlabel=None)\n",
    "\n",
    "sns.boxplot(x='mean_score', y ='weighting', data=cluster_df, hue='weighting', dodge=True, ax=axes[1])\n",
    "axes[1].set(ylabel=None)\n",
    "axes[1].set(xlabel='Mean Score by Cluster')\n",
    "axes[1].legend([], [], frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(5, 3)})\n",
    "fig, axes = plt.subplots(2, 1, sharey=True)\n",
    "\n",
    "sns.stripplot(x='var_score', y='weighting', data=cluster_df, jitter=True, hue='weighting', dodge=True, ax=axes[0])\n",
    "axes[0].legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "axes[0].set(ylabel=None)\n",
    "axes[0].set(xticklabels=[])\n",
    "axes[0].set(xlabel=None)\n",
    "\n",
    "sns.boxplot(x='var_score', y ='weighting', data=cluster_df, hue='weighting', dodge=True, ax=axes[1])\n",
    "axes[1].set(ylabel=None)\n",
    "axes[1].set(xlabel='Variance of Scores by Cluster')\n",
    "axes[1].legend([], [], frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_scores(df):\n",
    "  \n",
    "    cluster_columns = ['uw_kmeans', 'w_kmeans', 'iw_kmeans']\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    num_clusters = len(cluster_columns)\n",
    "    fig, axs = plt.subplots(num_clusters, 1, figsize=(8, 4*num_clusters))\n",
    "    \n",
    "    for i, cluster_column in enumerate(cluster_columns):\n",
    "        ax = axs[i]\n",
    "        sns.countplot(data=df, x=cluster_column, hue='task_flag', ax=ax)\n",
    "        ax.set_title(f'Score Counts by {cluster_column}')\n",
    "        ax.set_xlabel('Cluster')\n",
    "        ax.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_scores(dolly_cqa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_cohesion(cluster_embeddings, metric_cosine):\n",
    "    n = len(cluster_embeddings)\n",
    "    if n < 2:\n",
    "        return 0  # Cannot compute cohesion for a cluster with less than two points\n",
    "    if metric_cosine:\n",
    "        total_distance = sum(cosine(cluster_embeddings[i] / np.linalg.norm(cluster_embeddings[i]),\n",
    "                                    cluster_embeddings[j] / np.linalg.norm(cluster_embeddings[j])) for i in range(n) for j in range(i+1, n))\n",
    "    else:\n",
    "        total_distance = sum(euclidean(cluster_embeddings[i] / np.linalg.norm(cluster_embeddings[i]),\n",
    "                                       cluster_embeddings[j] / np.linalg.norm(cluster_embeddings[j])) for i in range(n) for j in range(i+1, n))\n",
    "    num_pairs = n * (n - 1) // 2  # Number of unique pairs in the cluster\n",
    "    return total_distance / num_pairs  # Normalize by the number of pairs\n",
    "\n",
    "def plot_cluster_cohesion(df, metric_cosine):\n",
    "    \n",
    "    cluster_columns = ['uw_kmeans', 'w_kmeans', 'iw_kmeans']\n",
    "    embedding_columns = ['embeddings_mini_lm', 'w_embeddings', 'iw_embeddings']\n",
    "\n",
    "    num_cluster_types = len(cluster_columns)\n",
    "    fig, axs = plt.subplots(num_cluster_types, 1, figsize=(6, 3*num_cluster_types))\n",
    "\n",
    "    for i, (cluster_column, embedding_column) in enumerate(zip(cluster_columns, embedding_columns)):\n",
    "        df['cohesion_'+cluster_column] = 0\n",
    "        for cluster_id in df[cluster_column].unique():\n",
    "            data = df[embedding_column][df[cluster_column] == cluster_id].reset_index(drop=True)\n",
    "            cohesion = _compute_cohesion(data, metric_cosine)\n",
    "            df['cohesion_'+cluster_column][df[cluster_column] == cluster_id] = cohesion\n",
    "        \n",
    "        ax = axs[i]\n",
    "        sns.barplot(data=df.groupby(cluster_column)['cohesion_'+cluster_column].mean().reset_index(), x=cluster_column, y='cohesion_'+cluster_column, ax=ax)\n",
    "        ax.set_title(f'Normalized Cohesion by {cluster_column}')\n",
    "        ax.set_xlabel('Cluster')\n",
    "        ax.set_ylabel('Cohesion')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_cohesion(dolly_cqa_df, metric_cosine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_separation(cluster_embeddings_1, cluster_embeddings_2, metric_cosine):\n",
    "    if metric_cosine:\n",
    "        total_distance = sum(cosine(point1 / np.linalg.norm(point1),\n",
    "                                    point2 / np.linalg.norm(point2)) for point1 in cluster_embeddings_1 for point2 in cluster_embeddings_2)\n",
    "    else:\n",
    "        total_distance = sum(euclidean(point1 / np.linalg.norm(point1),\n",
    "                                       point2 / np.linalg.norm(point2)) for point1 in cluster_embeddings_1 for point2 in cluster_embeddings_2)\n",
    "    num_pairs = len(cluster_embeddings_1) * len(cluster_embeddings_2)  # Number of pairs between the clusters\n",
    "    return total_distance / num_pairs  # Normalize by the number of pairs\n",
    "\n",
    "def plot_cluster_separation(df, metric_cosine):\n",
    "    \n",
    "    cluster_columns = ['uw_kmeans', 'w_kmeans', 'iw_kmeans']\n",
    "    embedding_columns = ['embeddings_mini_lm', 'w_embeddings', 'iw_embeddings']\n",
    "\n",
    "    num_cluster_types = len(cluster_columns)\n",
    "    fig, axs = plt.subplots(num_cluster_types, 1, figsize=(8, 4*num_cluster_types))\n",
    "\n",
    "    for n, (cluster_column, embedding_column) in enumerate(zip(cluster_columns, embedding_columns)):\n",
    "        num_clusters = len(df[cluster_column].unique())\n",
    "        separation_matrix = np.zeros((num_clusters, num_clusters))\n",
    "        \n",
    "        for i in range(num_clusters):\n",
    "            for j in range(i+1, num_clusters):\n",
    "                data_i = df[embedding_column][df[cluster_column] == i].reset_index(drop=True)\n",
    "                data_j = df[embedding_column][df[cluster_column] == j].reset_index(drop=True)\n",
    "                separation = _compute_separation(data_i, data_j, metric_cosine)\n",
    "                separation_matrix[i, j] = separation\n",
    "                separation_matrix[j, i] = separation  # The matrix is symmetric\n",
    "        \n",
    "        ax = axs[n]\n",
    "        sns.heatmap(separation_matrix, annot=True, cmap='viridis', fmt=\".2f\", linewidths=.5, ax=ax)\n",
    "        ax.set_title('Inter-Cluster Normalized Separation Heatmap for Cluster ' + cluster_column)\n",
    "        ax.set_xlabel('Cluster Index')\n",
    "        ax.set_ylabel('Cluster Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_separation(dolly_cqa_df, metric_cosine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(df, categories, embeddings, reduction='T-SNE'):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    if reduction == 'PCA':\n",
    "        reducer = PCA(n_components=3)\n",
    "    elif reduction == 'T-SNE':\n",
    "        reducer = TSNE(n_components=3)\n",
    "    else:\n",
    "        raise ValueError('Invalid dimensionality reduction method. Use \"PCA\" or \"T-SNE\".')\n",
    "\n",
    "    # Extract embeddings and apply reduction\n",
    "    embeddings = np.stack(df[embeddings].values)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Add reduced embeddings to dataframe\n",
    "    df['x'] = reduced_embeddings[:, 0]\n",
    "    df['y'] = reduced_embeddings[:, 1]\n",
    "    df['z'] = reduced_embeddings[:, 2]\n",
    "    df['categ'] = df[categories].astype(str)\n",
    "\n",
    "    # Create the 3D plot using plotly.express\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                        color='categ', opacity=0.5,\n",
    "                        size=[1] * len(df),\n",
    "                        width=800, height=600)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df=dolly_cqa_df, categories='uw_kmeans', embeddings='embeddings_mini_lm', reduction='PCA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df=dolly_cqa_df, categories='w_kmeans', embeddings='w_embeddings', reduction='PCA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df=dolly_cqa_df, categories='iw_kmeans', embeddings='iw_embeddings', reduction='PCA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in dolly_cqa_df['text'][dolly_cqa_df['iw_kmeans'] == 0]:\n",
    "    print(c, '\\n=============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another way I think could potentially give us better results in emphasizing or de-emphasizing an aspect of interest:\n",
    "\n",
    "1. Identify good sentences representative of the aspect of interest (e.g. a specific subject, or a particular type of question): We have this already.\n",
    "\n",
    "2. Instead of training a linear classifier or regressor, we create an embedding vector representative of that aspect: We could simply take the mean of all embedded sentences we identify in 1. We also create an embedding vector non-representative of that aspect in the same way using the sentences not flagged in 1.\n",
    "\n",
    "3. Re-weight the sentences embeddings: Here we need a way to modify the original sentence embeddings by adding or subtracting the influence of the desired aspect. We can do that in this way:\n",
    "\n",
    "    3.1. Project each sentence embedding vector onto the aspect embedding vectors computed earlier, creating \"positive\" and \"negative\" projections.\n",
    "\n",
    "    3.2 Add each projection vector to the corresponding sentence embedding (if we want to increase the aspect influence in the embeddings) or subtract each projection vector from the corresponding sentence embedding (if we want to decrease the aspect influence in the embeddings).\n",
    "\n",
    "    3.3 When creating weighted embedding vector, we do this in a way that we try to increase the influence of the \"positive\" projections and decrease the influence of \"negative\" projections into the sentences that we consider having the aspect of interest, and we also do the opposite for the sentences not having the aspect of interest.\n",
    "    \n",
    "    3.4 When creating inverse-weighted embedding vectors, we do the opposite we did for the weighted vectors.\n",
    "\n",
    "    3.5 Alpha is a hyperparameter controlling the influence of the projection vectors into the sentence embeddings. It could be optimized by searching it for the minimum entropy of the aspect by cluster when clustering with weighted vectors and at the same time searching for the maximum entropy of the aspect by cluster when clustering with inverse-weighted vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_cqa_df_2 = dolly_cqa_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_flag = 'task_flag'\n",
    "\n",
    "dolly_cqa_df_2['score'] = dolly_cqa_df_2[pattern_flag]\n",
    "\n",
    "pos_aspect_embedding = np.mean(dolly_cqa_df_2[dolly_cqa_df_2[pattern_flag] == True]['embeddings_mini_lm'])\n",
    "pos_aspect_embedding = pos_aspect_embedding / np.linalg.norm(pos_aspect_embedding)\n",
    "\n",
    "neg_aspect_embedding = np.mean(dolly_cqa_df_2[dolly_cqa_df_2[pattern_flag] == False]['embeddings_mini_lm'])\n",
    "neg_aspect_embedding = neg_aspect_embedding / np.linalg.norm(neg_aspect_embedding)\n",
    "\n",
    "dolly_cqa_df_2['w_embeddings'] = dolly_cqa_df_2['embeddings_mini_lm']\n",
    "dolly_cqa_df_2['iw_embeddings'] = dolly_cqa_df_2['embeddings_mini_lm']\n",
    "\n",
    "alpha = 1.25\n",
    "\n",
    "for i, embedding in enumerate(dolly_cqa_df_2['embeddings_mini_lm']):\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    pos_projection = np.dot(embedding, pos_aspect_embedding.T) * pos_aspect_embedding\n",
    "    pos_projection = pos_projection.reshape(-1,)\n",
    "    neg_projection = np.dot(embedding, neg_aspect_embedding.T) * neg_aspect_embedding\n",
    "    neg_projection = neg_projection.reshape(-1,)\n",
    "    if dolly_cqa_df_2[pattern_flag].iloc[i] == True:\n",
    "        dolly_cqa_df_2['w_embeddings'].iloc[i] = embedding + alpha * (pos_projection - neg_projection)\n",
    "        dolly_cqa_df_2['iw_embeddings'].iloc[i] = embedding - alpha * (pos_projection - neg_projection)\n",
    "    else:\n",
    "        dolly_cqa_df_2['w_embeddings'].iloc[i] = embedding + alpha * (neg_projection - pos_projection)\n",
    "        dolly_cqa_df_2['iw_embeddings'].iloc[i] = embedding - alpha * (neg_projection - pos_projection)\n",
    "\n",
    "seed=0\n",
    "uw_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\").fit( [v for v in dolly_cqa_df_2['embeddings_mini_lm']])\n",
    "w_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\").fit( [v for v in dolly_cqa_df_2['w_embeddings']])\n",
    "iw_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\").fit( [v for v in dolly_cqa_df_2['iw_embeddings']])\n",
    "\n",
    "dolly_cqa_df_2['uw_kmeans'] = uw_kmeans.labels_\n",
    "dolly_cqa_df_2['w_kmeans'] = w_kmeans.labels_\n",
    "dolly_cqa_df_2['iw_kmeans'] = iw_kmeans.labels_\n",
    "\n",
    "cluster_df = dolly_cqa_df_2[['uw_kmeans', 'w_kmeans', 'iw_kmeans', 'score']]\n",
    "cluster_df = pd.melt(cluster_df, id_vars=['score'], var_name='weighting', value_name='cluster_id')\n",
    "cluster_df = cluster_df.groupby([\"weighting\", \"cluster_id\"]).agg({'score': ['mean', 'var']}).reset_index()\n",
    "cluster_df.columns = ['weighting', 'cluster_id', 'mean_score', 'var_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(5, 3)})\n",
    "fig, axes = plt.subplots(2, 1, sharey=True)\n",
    "\n",
    "sns.stripplot(x='mean_score', y='weighting', data=cluster_df, jitter=True, hue='weighting', dodge=True, ax=axes[0])\n",
    "axes[0].legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "axes[0].set(ylabel=None)\n",
    "axes[0].set(xticklabels=[])\n",
    "axes[0].set(xlabel=None)\n",
    "\n",
    "sns.boxplot(x='mean_score', y ='weighting', data=cluster_df, hue='weighting', dodge=True, ax=axes[1])\n",
    "axes[1].set(ylabel=None)\n",
    "axes[1].set(xlabel='Mean Score by Cluster')\n",
    "axes[1].legend([], [], frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(5, 3)})\n",
    "fig, axes = plt.subplots(2, 1, sharey=True)\n",
    "\n",
    "sns.stripplot(x='var_score', y='weighting', data=cluster_df, jitter=True, hue='weighting', dodge=True, ax=axes[0])\n",
    "axes[0].legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "axes[0].set(ylabel=None)\n",
    "axes[0].set(xticklabels=[])\n",
    "axes[0].set(xlabel=None)\n",
    "\n",
    "sns.boxplot(x='var_score', y ='weighting', data=cluster_df, hue='weighting', dodge=True, ax=axes[1])\n",
    "axes[1].set(ylabel=None)\n",
    "axes[1].set(xlabel='Variance of Scores by Cluster')\n",
    "axes[1].legend([], [], frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_scores(dolly_cqa_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_cohesion(dolly_cqa_df_2, metric_cosine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_separation(dolly_cqa_df_2, metric_cosine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df=dolly_cqa_df_2, categories='uw_kmeans', embeddings='embeddings_mini_lm', reduction='PCA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df=dolly_cqa_df_2, categories='w_kmeans', embeddings='w_embeddings', reduction='PCA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df=dolly_cqa_df_2, categories='iw_kmeans', embeddings='iw_embeddings', reduction='PCA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in dolly_cqa_df_2['text'][dolly_cqa_df_2['iw_kmeans'] == 17]:\n",
    "    print(c, '\\n=============================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
