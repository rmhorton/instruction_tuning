{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from readability import Readability\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_with_embeddings(csv_path, emb_paths, emb_cols):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    for emb_path, emb_col in zip(emb_paths, emb_cols):\n",
    "        with open(emb_path, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        df[emb_col] = embeddings.tolist()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_readability(text):\n",
    "    r = Readability(text)\n",
    "    try:\n",
    "        fk = r.flesch_kincaid()\n",
    "        fk_score = fk.score\n",
    "    except:\n",
    "        fk_score = None\n",
    "\n",
    "    return fk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_frequency_binning(data, num_bins):\n",
    "    \"\"\"\n",
    "    Bucketize the data into bins with approximately equal number of data points.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (list or np.array): The data to be bucketized.\n",
    "    - num_bins (int): Number of bins desired.\n",
    "    \n",
    "    Returns:\n",
    "    - bins (list of tuples): List of intervals representing the bins.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = np.array(data)\n",
    "    \n",
    "    # Sort the data\n",
    "    sorted_data = np.sort(data)\n",
    "    \n",
    "    # Calculate bin edges using quantiles\n",
    "    bin_edges = [np.percentile(sorted_data, i) for i in np.linspace(0, 100, num_bins+1)]\n",
    "    \n",
    "    # Create bins as tuples of (start, end)\n",
    "    bins = [(bin_edges[i], bin_edges[i+1]) for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_data(data, bins, bin_names):\n",
    "    \"\"\"\n",
    "    Convert continuous data into categorical data using specified bins and bin names.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (list or np.array): The data to be bucketized.\n",
    "    - bins (list of tuples): List of intervals representing the bins.\n",
    "    - bin_names (list of str): Names for each bin.\n",
    "    \n",
    "    Returns:\n",
    "    - categorical_data (list of str): Categorical representation of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = np.array(data)\n",
    "    \n",
    "    if len(bins) != len(bin_names):\n",
    "        raise ValueError(\"Number of bins and bin names should be the same.\")\n",
    "    \n",
    "    # Initialize an empty list to store the categorical data\n",
    "    categorical_data = []\n",
    "    \n",
    "    # Loop over each data point to assign it to a bin\n",
    "    for value in data:\n",
    "        assigned = False\n",
    "        for i, (start, end) in enumerate(bins):\n",
    "            if start <= value < end or (i == len(bins) - 1 and value == end):\n",
    "                categorical_data.append(bin_names[i])\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            categorical_data.append('Unknown')\n",
    "    \n",
    "    return categorical_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_cqa_prepared_df = load_df_with_embeddings(csv_path='./data/dolly_cqa.csv',\n",
    "                                                emb_paths=['./data/embeddings_dolly_mini_lm.pickle', './data/embeddings_dolly_bge_large.pickle'],\n",
    "                                                emb_cols=['embeddings_mini_lm', 'embeddings_bge_large'])\n",
    "\n",
    "dolly_cqa_prepared_df['fk_score'] = dolly_cqa_prepared_df['text'].apply(lambda x: compute_readability(x))\n",
    "dolly_cqa_prepared_df = dolly_cqa_prepared_df.dropna()\n",
    "bins = equal_frequency_binning(dolly_cqa_prepared_df['fk_score'], num_bins=3)\n",
    "dolly_cqa_prepared_df['fk_score_categ'] = bin_data(dolly_cqa_prepared_df['fk_score'], bins, ['easy', 'medium', 'hard'])\n",
    "\n",
    "dolly_cqa_prepared_df.to_parquet('./data/dolly_cqa_prepared.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_cqa_prepared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_race_dataset(csv_path, emb_paths, emb_cols):\n",
    "    race_prepared_df = load_df_with_embeddings(csv_path, emb_paths, emb_cols)\n",
    "    race_prepared_df['fk_score'] = race_prepared_df['text'].apply(lambda x: compute_readability(x))\n",
    "    race_prepared_df = race_prepared_df.dropna()\n",
    "    bins = equal_frequency_binning(race_prepared_df['fk_score'], num_bins=3)\n",
    "    race_prepared_df['fk_score_categ'] = bin_data(race_prepared_df['fk_score'], bins, ['easy', 'medium', 'hard'])\n",
    "    return race_prepared_df\n",
    "\n",
    "race_train_df = prepare_race_dataset(csv_path='./data/race_train.csv',\n",
    "                                     emb_paths=['./data/embeddings_train_mini_lm.pickle', './data/embeddings_train_bge_large.pickle'],\n",
    "                                     emb_cols=['embeddings_mini_lm', 'embeddings_bge_large'])\n",
    "race_train_df.to_parquet('./data/race_train_prepared.parquet', index=False)\n",
    "\n",
    "race_validation_df = prepare_race_dataset(csv_path='./data/race_validation.csv',\n",
    "                                          emb_paths=['./data/embeddings_validation_mini_lm.pickle', './data/embeddings_validation_bge_large.pickle'],\n",
    "                                          emb_cols=['embeddings_mini_lm', 'embeddings_bge_large'])\n",
    "race_validation_df.to_parquet('./data/race_validation_prepared.parquet', index=False)\n",
    "\n",
    "race_test_df = prepare_race_dataset(csv_path='./data/race_test.csv',\n",
    "                                    emb_paths=['./data/embeddings_test_mini_lm.pickle', './data/embeddings_test_bge_large.pickle'],\n",
    "                                    emb_cols=['embeddings_mini_lm', 'embeddings_bge_large'])\n",
    "race_test_df.to_parquet('./data/race_test_prepared.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
